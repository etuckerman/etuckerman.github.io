<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaMA 3.1 QA Fine-Tune Project | Elliot Tuckerman</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>LLaMA 3.1 QA Fine-Tune Project</h1>
    </header>
    <main>
        <section id="project-overview">
            <h2>Project Overview</h2>
            <p>In this project, I fine-tuned the LLaMA 3.1 70B-Instruct model on a custom RV (Recreational Vehicle) dataset. The goal was to create a specialized model capable of answering detailed questions about various RV models, their features, and specifications.</p>
        </section>

        <section id="key-achievements">
            <h2>Key Achievements</h2>
            <ul>
                <li>Successfully fine-tuned a large language model (70B parameters) for a specific domain</li>
                <li>Implemented advanced fine-tuning techniques using PEFT, LoRA, and Unsloth</li>
                <li>Optimized the model for efficient inference on limited hardware</li>
                <li>Deployed the fine-tuned model to Hugging Face Hub for easy access and integration</li>
            </ul>
        </section>

        <section id="skills-used">
            <h2>Skills and Technologies Used</h2>
            <ul>
                <li>Python programming</li>
                <li>PyTorch and Hugging Face Transformers library</li>
                <li>Large Language Model fine-tuning techniques</li>
                <li>PEFT (Parameter-Efficient Fine-Tuning)</li>
                <li>LoRA (Low-Rank Adaptation)</li>
                <li>Unsloth for optimization</li>
                <li>Data preprocessing and formatting</li>
                <li>Model deployment to Hugging Face Hub</li>
            </ul>
        </section>

        <section id="challenges-solutions">
            <h2>Challenges and Solutions</h2>
            <h3>Challenge 1: Limited Computational Resources</h3>
            <p>Fine-tuning a 70B parameter model requires significant computational power. To overcome this, I implemented several optimization techniques:</p>
            <ul>
                <li>Used 4-bit quantization to reduce memory usage</li>
                <li>Implemented LoRA for parameter-efficient fine-tuning</li>
                <li>Utilized Unsloth for optimized training, reducing VRAM usage by 30%</li>
            </ul>

            <h3>Challenge 2: Custom Dataset Formatting</h3>
            <p>The RV dataset needed specific formatting for effective fine-tuning. I created a custom formatting function to prepare the data:</p>
            <pre><code>
def formatting_prompts_func(examples):
    questions = examples["Question"]
    answers = examples["Answer"]
    texts = []
    for question, answer in zip(questions, answers):
        text = alpaca_prompt.format(question, answer) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}
            </code></pre>
        </section>

        <section id="key-learnings">
            <h2>Key Learnings</h2>
            <ul>
                <li>Gained deep understanding of large language model fine-tuning processes</li>
                <li>Learned to balance model performance with computational efficiency</li>
                <li>Improved skills in data preprocessing for NLP tasks</li>
                <li>Developed expertise in using advanced libraries like PEFT and Unsloth</li>
                <li>Enhanced ability to deploy and share models through platforms like Hugging Face Hub</li>
            </ul>
        </section>

        <section id="future-improvements">
            <h2>Future Improvements</h2>
            <ul>
                <li>Expand the dataset to cover a wider range of RV models and specifications</li>
                <li>Implement a more sophisticated prompt engineering technique</li>
                <li>Explore multi-task fine-tuning to improve the model's general knowledge while maintaining RV expertise</li>
                <li>Develop a user-friendly interface for non-technical users to interact with the model</li>
            </ul>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>This project demonstrated the power of fine-tuning large language models for specific domains. By successfully adapting the LLaMA 3.1 70B-Instruct model to the RV industry, I've created a valuable tool that can provide accurate and detailed information about various RV models. This experience has significantly enhanced my skills in NLP, model optimization, and deployment of AI solutions.</p>
        </section>

        <section id="training-process">
            <h2>Training Process and Results</h2>
            <p>The fine-tuning process was executed using Unsloth, an optimization library that promises 2x faster free fine-tuning. Here's an overview of the training configuration and results:</p>
            
            <h3>Training Configuration</h3>
            <ul>
                <li>Number of GPUs: 1</li>
                <li>Number of examples: 16</li>
                <li>Number of Epochs: 30</li>
                <li>Batch size per device: 2</li>
                <li>Gradient Accumulation steps: 4</li>
                <li>Total batch size: 8</li>
                <li>Total steps: 60</li>
                <li>Number of trainable parameters: 207,093,760</li>
            </ul>

            <h3>Training Progress</h3>
            <p>The training process completed 60 steps over 30 epochs in approximately 5 minutes and 51 seconds. This demonstrates the efficiency of the Unsloth optimization.</p>

            <h3>Loss Curve</h3>
            <div id="loss-chart"></div>
            <p>The training loss decreased significantly over the course of training:</p>
            <ul>
                <li>Initial loss: 2.5866</li>
                <li>Final loss: 0.0692</li>
            </ul>
            <p>This substantial reduction in loss indicates that the model successfully learned from the RV-specific dataset, improving its ability to generate accurate responses to RV-related queries.</p>

            <h3>Key Observations</h3>
            <ul>
                <li>Rapid Initial Improvement: The loss dropped sharply in the first 10 steps, from 2.5866 to 0.4676, showing quick adaptation to the task.</li>
                <li>Steady Refinement: After the initial drop, the loss continued to decrease more gradually, reaching below 0.1 by step 32.</li>
                <li>Stability in Later Steps: From step 40 onwards, the loss stabilized around 0.07, indicating that the model had reached a good level of performance and was fine-tuning more subtle aspects of its knowledge.</li>
            </ul>

            <h3>Implications</h3>
            <p>The training results suggest that our fine-tuned model has successfully adapted to the RV domain. The low final loss indicates that the model should be capable of generating accurate and relevant responses to RV-related questions. However, it's important to note that while a low training loss is a good indicator, the true test of the model's performance will be in its ability to generalize to new, unseen questions about RVs.</p>
        </section>

        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script>
            var trace = {
                x: Array.from({length: 60}, (_, i) => i + 1),
                y: [2.586600, 2.506600, 2.499600, 2.407300, 2.102200, 1.590100, 1.227700, 0.933100, 0.661400, 0.467600, 0.341900, 0.370700, 0.326500, 0.327400, 0.287500, 0.279000, 0.270100, 0.257100, 0.247100, 0.243600, 0.236200, 0.224700, 0.219000, 0.204400, 0.202200, 0.180100, 0.169000, 0.164400, 0.144000, 0.133300, 0.117800, 0.098900, 0.089300, 0.077600, 0.073200, 0.070500, 0.069800, 0.070600, 0.067900, 0.072500, 0.070300, 0.071400, 0.073800, 0.068900, 0.070700, 0.070600, 0.070500, 0.069700, 0.069500, 0.068600, 0.068800, 0.069500, 0.067700, 0.070200, 0.065700, 0.071000, 0.066500, 0.070200, 0.067400, 0.069200],
                type: 'scatter'
            };

            var layout = {
                title: 'Training Loss Over Steps',
                xaxis: {title: 'Step'},
                yaxis: {title: 'Loss'}
            };

            Plotly.newPlot('loss-chart', [trace], layout);
        </script>
    </main>
    <footer>
        <p><a href="index.html">Back to Portfolio</a></p>
    </footer>
</body>
</html>