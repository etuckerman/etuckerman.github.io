<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLaMA 3.1 QA Fine-Tune Project | Elliot Tuckerman</title>
    <link rel="stylesheet" href="../css/styles.css">
    <script src="../js/script.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body>
    <main class="project-container">
        <h1>LLaMA 3.1 Fine-Tuning for RV Q&A</h1>
        
        <section id="situation">
            <h2>Situation</h2>
            <p>The recreational vehicle (RV) market lacks an efficient, AI-powered question-answering system to provide quick and accurate information about various RV models.</p>
        </section>

        <section id="task">
            <h2>Task</h2>
            <p>Develop a specialized AI model capable of answering detailed questions about different RV models, including specifications, features, and pricing.</p>
        </section>

        <section id="action">
            <h2>Action</h2>
            <ol>
                <li>Prepared a dataset of RV-specific questions and answers.</li>
                <li>Fine-tuned the LLaMA 3.1 70B Instruct model using the Unsloth library for efficient training.</li>
                <li>Implemented LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.</li>
                <li>Optimized the training process for speed and memory efficiency.</li>
            </ol>
            
            <h3>Training Configuration</h3>
            <ul>
                <li>Model: LLaMA 3.1 70B Instruct</li>
                <li>Max Sequence Length: 2048 tokens</li>
                <li>LoRA rank: 16</li>
                <li>Training Steps: 60</li>
                <li>Batch Size: 2 (8 with gradient accumulation)</li>
                <li>Learning Rate: 2e-4</li>
            </ul>

            <div id="training-loss-chart"></div>
        </section>

        <section id="result">
            <h2>Result</h2>
            <p>The fine-tuned model demonstrates impressive capabilities in answering RV-specific questions:</p>
            
            <div class="qa-examples">
                <h3>Sample Q&A</h3>
                <div class="qa-pair">
                    <p><strong>Q: Does the Jayco Eagle have a queen size bed?</strong></p>
                    <p>A: Yes, the Jayco Eagle has 1 queen size bed.</p>
                </div>
                <div class="qa-pair">
                    <p><strong>Q: What is the MSRP of the Keystone Cougar?</strong></p>
                    <p>A: The MSRP of the Keystone Cougar is $28,603.</p>
                </div>
                <!-- Add more Q&A pairs here -->
            </div>

            <h3>Performance Metrics</h3>
            <ul>
                <li>Training Time: 6.0 minutes</li>
                <li>Peak Memory Usage: 40.941 GB</li>
                <li>Memory Usage for Training: 2.082 GB (2.63% of available GPU memory)</li>
            </ul>

            <p>The model has been successfully uploaded to Hugging Face Hub and is available for further use and experimentation.</p>
        </section>
    </main>

    <script>
        // Training loss chart
        var trace = {
            x: [1, 10, 20, 30, 40, 50, 60],
            y: [2.586600, 0.467600, 0.243600, 0.133300, 0.072500, 0.068600, 0.069200],
            type: 'scatter'
        };

        var layout = {
            title: 'Training Loss Over Time',
            xaxis: {title: 'Step'},
            yaxis: {title: 'Loss'},
            plot_bgcolor: '#2a2a2a',
            paper_bgcolor: '#2a2a2a',
            font: {color: '#e0e0e0'},
            margin: {t: 30, b: 40, l: 60, r: 10}
        };

        Plotly.newPlot('training-loss-chart', [trace], layout, {responsive: true});
    </script>
</body>
</html>